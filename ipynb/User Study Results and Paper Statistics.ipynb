{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csv files for answer responses\n",
    "gdrive_path = \"/Users/dorislee/Google Drive/Turn/user_study/evaluation_study/transcription_and_analysis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_task =  pd.read_csv(gdrive_path+\"UserTaskAssignment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from paper_visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dcg(r, k, method=0):\n",
    "    # alternative formulation of DCG places stronger emphasis on retrieving relevant documents\n",
    "    r = np.asfarray(r)[:k]\n",
    "    val = 0\n",
    "    for i in range(1,len(r)):\n",
    "        val+= (2**r[i]-1) / np.log2(i+1)\n",
    "    return val\n",
    "\n",
    "def ndcg(dcg_ground_truth,r, k):\n",
    "    return dcg(r, k) / sum(dcg_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ndcg_rankings(user_ranking_filename,gt_ranking_filename):#,k_lst = [1,2,3,5,7]):\n",
    "\n",
    "    user_rank = pd.read_csv(gdrive_path+user_ranking_filename)\n",
    "    ground_truth_rank = pd.read_csv(gt_ranking_filename,index_col=0)\n",
    "    k = len(ground_truth_rank[\"attribute\"])\n",
    "    ground_truth_cramerV_rank = list(ground_truth_rank[\"cramerV\"].rank(ascending=False)) \n",
    "    #print ground_truth_cramerV_rank\n",
    "    if \"Police\" in user_ranking_filename: \n",
    "        task_name = \"Task1\"\n",
    "    else: \n",
    "        task_name = \"Task2\"\n",
    "    #highest cramer's V corresponds to most correlated, corresponds to rank 1\n",
    "    user_rank = user_rank.merge(user_task).drop(\"Note\",axis=1)\n",
    "    user_rankings=[]\n",
    "    #Putting user rankings into appropriate data structures\n",
    "    for row in user_rank.iterrows():\n",
    "        ranking = []\n",
    "        for attr in ground_truth_rank.attribute:\n",
    "            ranking.append(row[1][attr])\n",
    "        task = row[1][task_name]\n",
    "        user_rankings.append([task,ranking])\n",
    "    #Computing ndcg ranking\n",
    "    ndcg_data = []\n",
    "    for trial in user_rankings:\n",
    "        ndcg_d = []\n",
    "        #print trial\n",
    "        ndcg_d.append(trial[0])\n",
    "        #for k in k_lst:\n",
    "        ndcg_d.append(ndcg(ground_truth_cramerV_rank,trial[1],k))\n",
    "#             print trial[1]\n",
    "        ndcg_data.append(ndcg_d)\n",
    "#    print ndcg_data\n",
    "    NDCG_lst = [\"NDCG@\"+str(k)]\n",
    "    ndcg_result = pd.DataFrame(ndcg_data,columns=[\"Task\"]+NDCG_lst)\n",
    "    return ndcg_result.groupby(\"Task\",as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ranking_filename = \"Ranking(Police).csv\"\n",
    "gt_ranking_filename = \"ct_police_ground_truth_correlation.csv\"\n",
    "user_rank = pd.read_csv(gdrive_path+user_ranking_filename)\n",
    "ground_truth_rank = pd.read_csv(gt_ranking_filename,index_col=0)\n",
    "k = len(ground_truth_rank[\"attribute\"])\n",
    "ground_truth_cramerV_rank = list(ground_truth_rank[\"cramerV\"].rank(ascending=False)) \n",
    "#print ground_truth_cramerV_rank\n",
    "if \"Police\" in user_ranking_filename: \n",
    "    task_name = \"Task1\"\n",
    "else: \n",
    "    task_name = \"Task2\"\n",
    "#highest cramer's V corresponds to most correlated, corresponds to rank 1\n",
    "user_rank = user_rank.merge(user_task).drop(\"Note\",axis=1)\n",
    "user_rankings=[]\n",
    "#Putting user rankings into appropriate data structures\n",
    "for row in user_rank.iterrows():\n",
    "    ranking = []\n",
    "    for attr in ground_truth_rank.attribute:\n",
    "        ranking.append(row[1][attr])\n",
    "    task = row[1][task_name]\n",
    "    user_rankings.append([task,ranking])\n",
    "#Computing ndcg ranking\n",
    "ndcg_data = []\n",
    "for trial in user_rankings:\n",
    "    ndcg_d = []\n",
    "    #print trial\n",
    "    ndcg_d.append(trial[0])\n",
    "    #for k in k_lst:\n",
    "    ndcg_d.append(ndcg(ground_truth_cramerV_rank,trial[1],k))\n",
    "#             print trial[1]\n",
    "    ndcg_data.append(ndcg_d)\n",
    "#    print ndcg_data\n",
    "NDCG_lst = [\"NDCG@\"+str(k)]\n",
    "ndcg_result = pd.DataFrame(ndcg_data,columns=[\"Task\"]+NDCG_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_result=[[\"Police\"]+list(compute_ndcg_rankings(\"Ranking(Police).csv\",\"ct_police_ground_truth_correlation.csv\")[\"NDCG@7\"]),\n",
    "[\"Autism\"]+list(compute_ndcg_rankings(\"Ranking(Autism).csv\",\"autism_ground_truth_correlation.csv\")[\"NDCG@10\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\hline\n",
      " Dataset   &   \\system &   Cluster &   BFS \\\\\n",
      "\\hline\n",
      " Police    &      0.63 &      0.45 &  0.84 \\\\\n",
      " Autism    &      0.50 &      0.35 &  0.24 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "print (T.tabulate(ndcg_result,headers=['Dataset','\\system','Cluster','BFS'],tablefmt='latex', floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NDCG@1 is 0 because if the first-ranked attribute is not retreived, then score would be 0.\n",
    "- This makes sense, our algo does better than kmeans (2) and level-wise BFS (3), except for NDCG@3 A2 for some reason did very well.\n",
    "- NDCG should be @10 because all users read through and ranked all 10 dashboards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
