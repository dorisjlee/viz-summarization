\newpage
\newpage
\section{Preliminaries: Data and User Models} 
%In this section, we first describe the data model, and the supported visualization types. We then introduce two visualization spaces, and propose a user expectation model to traverse these spaces. Finally, we formally define the problem.

In this section, we first describe our data model by reporting our data, visualization and query setup, and the underlying lattice of data subsets. We then introduce our user model to traverse the lattice and justify its efficacy. Finally, we present our problem of finding informative and interesting visualizations from the lattice.

%\subsection{Visualizations and Queries}

\subsection{Data Model}
We start by discussing the data, visualization and query setup.

\textbf{Data.} We assume that we are operating on a dataset consisting of a single relational table $R$. Our approach also generalizes to multiple tables obeying a star or a snowflake schema, but we focus on the single table case for ease of presentation.

%\notes{[It seems that the visualization types that can be supported by our framework/system is similar to \textsc{Zenvisage}. In fact, we can use either \textsc{SQL} or \textsc{ZQL} as back-end.]} 

%\textbf{Visualization.} We define the notion of a visualization for our problem. There are different visualization types such as bar charts, scatter plots, and trend lines, but across all types, a visualization can be defined using five main components: (i) the x-axis attribute(s), (ii) the y-axis attribute, (iii) the subset of data used, (iv) the binning and aggregation functions for the x- and y- axes, and (v) the type of visualization (e.g., bar chart, scatter plot).

\textbf{Visualization.} We define the notion of a visualization for our problem. In our setup, a visualization can be defined using five main components: (i) the x-axis attributes, (ii) the y-axis attribute, (iii) the subset of data used, (iv) the binning and aggregation functions for the x- and y- axes, and (v) the type of visualization, e.g., bar chart.


\textbf{Query.} Our query setup applies to all visualization types that can be defined in terms of the aforementioned components, including common visualization types such as bar charts, trend lines, and scatter plots. Visualizations of these types can be translated into the following \textsc{SQL} query: {\tt SELECT X, A(Y) FROM R WHERE C(Z) GROUP BY X}. Here, $X$ refers to the x-axis attribute(s), $Y$ to the y-axis attribute, $A(Y)$ to the aggregation function on $Y$, $Z$ to the attribute(s) used in specifying the data subset, and $C(Z)$ to the constraints or filters that specify the data subset.

Now, we discuss the set-theory-based \emph{containment} relationships for data subsets, and based on the relationships define our lattice.

\textbf{Containment.} Given a data subset defined by a set of constraints $C = \{z_1, \ldots, z_n\}$, expanding $C$ by adding one or more new constraints will generate a new data subset that is contained within the former subset. An ancestor-descendant relationship exists between these data subsets. Specifically, a data subset defined by constraints $C_a$ is an ancestor of a data subset defined by constraints $C_b$, if and only if $C_b \subsetneqq C_a$. Further, a data subset defined by constraints $C_a$ is a parent of a data subset defined by constraints $C_b$, if and only if $C_b \subsetneqq C_a \land \mid C_a \mid - \mid C_b \mid = 1$. For example, given a dataset with three binary attributes {\tt \{P, Q, R\}}, the data subset defined by constraints {\tt \{P = 0,Q = 1\}} has two parents--- the data subset defined by constraint {\tt \{P = 0\}} and the data subset defined by constraint {\tt \{Q = 1\}}.

\textbf{Lattice.} Based on the containment relationship, we can organize the data subsets of a given dataset to form a lattice. This lattice of data subsets is in fact a hierarchy, which is contained within the lattice of attribute combinations. In data cube literature, attribute combinations are called cuboids, and the data subsets are called cells. In Figure 1 we show the lattice of data subsets for a dataset with three binary attributes {\tt \{P, Q, R\}}, where data subsets that belong to the same attribute combination are highlighted with same color. For example, all data subsets with three filters (at the lowest level of hierarchy) belong to the same attribute combination {\tt \{P, Q, R\}}. 

\begin{figure}[htb]
\begin{tikzpicture}[scale=0.75, every node/.style={scale=0.75}]
  \node[fill={rgb:black,1;white,14}] (max) at (0,4.5) {\tt \{P = ?,Q = ?,R = ?\}};
  \node[fill={rgb:red,1;white,4}] (11) at (-4.0,3) {\tt \{P = 0\}};
  \node[fill={rgb:red,1;white,4}] (12) at (-2.4,3) {\tt \{P = 1\}};
  \node[fill={rgb:green,1;white,4}] (13) at (-0.8,3) {\tt \{Q = 0\}};
  \node[fill={rgb:green,1;white,4}] (14) at (0.8,3) {\tt \{Q = 1\}};
  \node[fill={rgb:blue,1;white,4}] (15) at (2.4,3) {\tt \{R = 0\}};
  \node[fill={rgb:blue,1;white,4}] (16) at (4.0,3) {\tt \{R = 1\}};
  \node[fill={rgb:yellow,1;white,4}]  (21) at (-4,1.5) {\tt \{P = 0,Q = 0\}};
  \node[fill={rgb:yellow,1;white,4}]  (22) at (-1.5,1.5) {\tt \{P = 0,Q = 1\}};
  \node (2f) at (0,1.5) {$\ldots$};
  \node[fill={rgb:cyan,1;white,4}] (23) at (1.5,1.5) {\tt \{Q = 1,R = 0\}};
  \node[fill={rgb:cyan,1;white,4}] (24) at (4,1.5) {\tt \{Q = 1,R = 1\}};
  \node[fill={rgb:black,1;white,4}] (31) at (-4,0) {\tt \{P = 0,Q = 0,R = 0\}};
  \node (3f) at (0,0) {$\ldots$};
  \node[fill={rgb:black,1;white,4}] (32) at (4,0) {\tt \{P = 1,Q = 1,R = 1\}};
  \draw (max) -- (11) (max) -- (12) (max) -- (13) (max) -- (14) (max) -- (15) (max) -- (16)
  (11) -- (21) (13) -- (21) (11) -- (22) (14) -- (22) (14) -- (23) (15) -- (23) (14) -- (24) (16) -- (24)
  (21) -- (31) (24) -- (32);
\end{tikzpicture}
\caption{Lattice of data subsets for a dataset with three binary attributes {\tt \{P, Q, R\}}. The data subsets (called cells in datacubes) that belong to the same attribute combination (cuboid) are highlighted with same color.}
\label{fig:lattice}
\end{figure}

We now extend the concept of \emph{containment} relationships (of data subsets) for visualizations defined according to our setup. 

\textbf{Viz Containment.} Let $V$ be the set of visualizations (of same type) that show the same $X$ and $Y$ for different $C(Z)$. Given a visualization $V_i \in V$, parent of $V_i$, $V_i^j$ ($V_i^j\in V$) is a visualization that corresponds to a parent data subset of the former. In Figure 2 we present three visualizations that show the average price of products bought by male and female customers for three data subsets: (i) US citizens, (ii) people who buy products in Jan, (iii) US citizens who buy products in Jan. As per the parent-child relationship among the data subsets, the visualizations corresponding to (i) and (ii) are parents of the visualization corresponding to (iii).

\begin{figure}[bht]
\label{example}
\centering
\includegraphics[scale=0.5]{figures/SubsetRelation.pdf}
\caption{Parent-child relationship amongst visualizations that show the average price of products bought by male and female customers for three data subsets: (i) US citizens, (ii) people who buy products in Jan, (iii) US citizens who buy products in Jan.}
\end{figure}



\iffalse
\begin{figure}[bht]
\label{example}
\centering
\includegraphics[scale=0.5]{figures/DimensionRelation.pdf}
\caption{Parent Child Relationship in Dimension Combination Space}
\end{figure}

\textbf{Dimension Combination Space.} Given a combination of dimension attributes $X = \{X_1, \ldots, X_n\}$, adding one or more new dimensions in $X$ will generate a new combination.  An ancestor-descendant relationship exists between these dimension combinations. Specifically, a dimension combination $X_a$ is an ancestor of a dimension combination $X_b$, if and only if $X_b \subsetneqq X_a$. Further,  a dimension combination $X_a$ is a parent of a dimension combination $X_b$, if and only if $X_b \subsetneqq X_a \land \mid X_a \mid - \mid X_b \mid = 1$. Notice that, a dimension combination can have multiple parent combinations. For example, given a sales dataset with four attributes {\tt\{Country, Gender, Month, Price\}}, the dimension combination {\tt\{Country, Month\}} has two parent combinations--- {\tt\{Country\}} and {\tt\{Month\}}. 

We now extend the concept of parent dimension combination to introduce the idea of parent visualization. Let $U$ be the set of visualizations (of same type) that show the same $Y$ and $C(Z)$ for different $X$. Given a visualization $V_i \in U$, parent of $V_i$, $V_i^j$ ($V_i^j\in U$) is a visualization that corresponds to a parent dimension combination of the former. Analogous to a dimension combination, a visualization $V_i \in U$ can have multiple parents. Figure 2 presents three visualizations that show the average price of products for three different dimension combinations: (i) US citizens/non-citizens, (ii) males/females, (iii) males/females who are US citizens/non-citizens. As per the parent-child relationship between the dimension combinations, the visualizations corresponding to (i) and (ii) are parents of the visualization corresponding to (iii).
\fi

\subsection{User Expectation Model}
In order to understand which visualizations a user would deem ``informative" and ``interesting" , we consider how analysts explore their data through OLAP operations (drill-down and roll-ups), and model the effective utility of displaying an unseen visualization to a user in the context of other seen visualizations. 

\textbf{Top Down Traversal.} We observe that, while exploring a dataset, users first look at the top level visualizations before looking at the next level. Further, the data distributions learnt from the top level visualizations set user expectation for the next level. Based on these two observations, we model user expectation $\hat{V_i}$ corresponding to an unseen visualization $V_i$ based on its seen/observed parents, $P(V_i) = \{V_i^1, \ldots, V_i^\lambda\}$. Specifically, we model two aspects of each unseen-visualization and observed-parent relationship, namely informativeness and interestingness. 

\textbf{Informativeness.} To model the informativeness of an observed parent in the context of an unseen visualization, we characterize the capability of the parent in predicting the unseen visualization. For an unseen visualization, the observed parents whose data distributions accurately predict the data distribution of the unseen are \emph{informative}. Specifically, we formulate the informativeness of an observed parent $V_i^j$ ($V_i^j \in P(V_i)$) of an unseen visualization $V_i$ as the similarity between their data distributions using a similarity function $S(V_i, V_i^j)$. The most informative parents $V_i^*$ of an unseen visualization $V_i$ are the ones whose data distributions have highest similarity with that of the unseen.

\begin{equation}
    V_i^*=\underset{V_i^j}{argmax}\ S(V_i, V_i^j)
\end{equation}

Since declaring a parent as informative can vary depending on its similarity with the unseen compared to other parents, we allow the user to specify a threshold $\theta$, which we expect to be close to one, such that the similarity score $S(V_i, V_i^{*, \theta})$ corresponding to the informative parents $V_i^{*, \theta}$ are at least $\theta$ fraction of that of the most informative parents'.

\begin{equation}
    V_i^{*, \theta} = \{V_i^j : \frac{S(V_i, V_i^j)}{S(V_i, V_i^*)} \ge \theta\}
\end{equation}

\textbf{Interestingness.} While the informative parents contribute to the prediction of an unseen visualization, the most interesting visualizations to recommend are those for which even the informative parents fail to predict the visualization. To model the interestingness of an unseen visualization $V_i$ in the context of an observed parent $V_i^j$ ($V_i^j \in P(V_i)$), we characterize the deviation between their data distributions using a distance function $D(V_i, V_i^j)$. The unseen visualizations whose data distributions deviate from the observed informative parents are \emph{interesting}. The most interesting unseen visualizations $V_\#$ are the ones that deviate most from their observed informative parents.
\begin{equation}
    V_\#=\underset{V_i}{argmax} \ D(V_i, V_i^{*, \theta})
\end{equation}

%\noindent Additional model extensions can be added to this objective function based user specification. For example, there may be $k$ visualizations that approximately yield equal contribution to the user's expectation. For simplicity of notation, we have assumed $k=1$ in the aforementioned model. In order, a user may want to prevent the recommendation of spuriously interesting subsets of the data. We can discard visualizations that falls below a certain subpopulation size threshold. 

\subsection{Problem Statement}

Given a dataset and user specified x- and y- axes, the goal of our system is to generate a dashboard by selecting $k$ visualizations from the data subset space, where (i) one of the $k$ visualization is the root--- the visualization corresponding to the entire dataset with no constraints, (ii) for each visualization except for the root, at least one of its informative parents is included within the $k$ visualizations, (iii) the $k$ visualizations are collectively most \lq\lq interesting\rq\rq\ in presence of their informative parents, (iv) the number of latent visualizations (not present in dashboard) whose informative parents are present in the dashboard is maximal.

